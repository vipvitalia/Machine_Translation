{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "GRU_ru-en_with_time.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LAfE2SHft3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "881913a2-f6ae-42c6-e850-b836ba91fda3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVPOFaaZ38qB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "210f0590-bb4e-47c2-ebf0-364bf226fb80"
      },
      "source": [
        "!git clone https://github.com/wl-research/nubia.git\n",
        "import os\n",
        "os.chdir('nubia')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nubia'...\n",
            "remote: Enumerating objects: 272, done.\u001b[K\n",
            "remote: Total 272 (delta 0), reused 0 (delta 0), pack-reused 272\u001b[K\n",
            "Receiving objects: 100% (272/272), 70.09 MiB | 19.31 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVxCYrjn4ETH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "f5895f06-378f-4c42-9fcc-c2d65382b5e1"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.5.0+cu101)\n",
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 6.8MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.15.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (0.29.19)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 3)) (1.13.23)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.2->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq->-r requirements.txt (line 2)) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.23 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (1.16.23)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.23->boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.23->boto3->pytorch-transformers->-r requirements.txt (line 3)) (2.8.1)\n",
            "Building wheels for collected packages: fairseq, wget, sacremoses\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2021167 sha256=ca3eafc5fee3b8362ecb4736f180f8f5a42bb93dd420679beea825120d94e33b\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=81078fb1dd40ae24f3401d598c51383a94689683b27b4858c8b33231719200f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=340d778fbdf0717c2204e4bbaa804aeec3033023cab49c95346f8ae6720d3c8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built fairseq wget sacremoses\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq, sentencepiece, sacremoses, pytorch-transformers, wget\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 pytorch-transformers-1.2.0 sacrebleu-1.4.10 sacremoses-0.0.43 sentencepiece-0.1.91 wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:18:54.691402Z",
          "start_time": "2020-06-15T12:18:46.254594Z"
        },
        "id": "ldJDWGq8fpqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "c57e2c5d-ece6-44a9-ffff-b6b1800dbbe5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, GRU, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from nubia import Nubia\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZi7JvJt4aAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "a1707a10-bbde-4a08-bb63-bb5fd1324906"
      },
      "source": [
        "nubia = Nubia()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading archive file pretrained/roBERTa_STS\n",
            "| [input] dictionary: 50265 types\n",
            "loading archive file pretrained/roBERTa_MNLI\n",
            "| dictionary: 50264 types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:18:55.545056Z",
          "start_time": "2020-06-15T12:18:55.396848Z"
        },
        "id": "VCCe4UPRfpqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines= pd.read_table('/content/drive/My Drive/rus.txt', names=['eng','rus', '1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0l0PDFOhOWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = lines.drop(['1'], axis=1)\n",
        "lines = lines[:38696]\n",
        "lines = shuffle(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:18:56.528523Z",
          "start_time": "2020-06-15T12:18:56.519999Z"
        },
        "id": "HgUrhWcffpqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "94104202-80ef-4609-f253-a1000df4fd95"
      },
      "source": [
        "lines.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38696, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:18:57.459215Z",
          "start_time": "2020-06-15T12:18:57.405476Z"
        },
        "id": "ipJ7WZoJfpqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lowercase all characters\n",
        "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
        "lines.rus=lines.rus.apply(lambda x: x.lower())\n",
        "\n",
        "# Remove quotes\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines.rus=lines.rus.apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "# Remove all the special characters\n",
        "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines.rus=lines.rus.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "# Remove all numbers from text\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines.eng = lines.eng.apply(lambda x: x.translate(remove_digits))\n",
        "lines.rus = lines.rus.apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "# Remove extra spaces\n",
        "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
        "lines.rus=lines.rus.apply(lambda x: x.strip())\n",
        "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines.rus=lines.rus.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "# Add start and end tokens to target sequences\n",
        "lines.eng = lines.eng.apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:01.984516Z",
          "start_time": "2020-06-15T12:19:01.965691Z"
        },
        "id": "wvelqp93fpqu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "00304a8f-e4fb-4c00-bcec-dd0285dc09e6"
      },
      "source": [
        "lines.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>rus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34731</th>\n",
              "      <td>START_ who did you call _END</td>\n",
              "      <td>кому ты звонил</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15323</th>\n",
              "      <td>START_ ill just wait _END</td>\n",
              "      <td>я просто подожду</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13829</th>\n",
              "      <td>START_ dont struggle _END</td>\n",
              "      <td>не деритесь</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10315</th>\n",
              "      <td>START_ ill save tom _END</td>\n",
              "      <td>я спасу тома</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19600</th>\n",
              "      <td>START_ dont forget it _END</td>\n",
              "      <td>не забывай об этом</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31789</th>\n",
              "      <td>START_ someone screamed _END</td>\n",
              "      <td>ктото завизжал</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4907</th>\n",
              "      <td>START_ tom is nuts _END</td>\n",
              "      <td>том не дружит с головой</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29000</th>\n",
              "      <td>START_ i like surprises _END</td>\n",
              "      <td>я люблю сюрпризы</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32020</th>\n",
              "      <td>START_ thats mine too _END</td>\n",
              "      <td>это тоже моё</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28808</th>\n",
              "      <td>START_ i have questions _END</td>\n",
              "      <td>у меня есть вопросы</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                eng                      rus\n",
              "34731  START_ who did you call _END           кому ты звонил\n",
              "15323     START_ ill just wait _END         я просто подожду\n",
              "13829     START_ dont struggle _END              не деритесь\n",
              "10315      START_ ill save tom _END             я спасу тома\n",
              "19600    START_ dont forget it _END       не забывай об этом\n",
              "31789  START_ someone screamed _END           ктото завизжал\n",
              "4907        START_ tom is nuts _END  том не дружит с головой\n",
              "29000  START_ i like surprises _END         я люблю сюрпризы\n",
              "32020    START_ thats mine too _END             это тоже моё\n",
              "28808  START_ i have questions _END      у меня есть вопросы"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:02.940437Z",
          "start_time": "2020-06-15T12:19:02.677135Z"
        },
        "id": "zkWxdPIMfpqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vocabulary of Russian\n",
        "all_rus_words=set()\n",
        "for rus in lines.rus:\n",
        "    for word in rus.split():\n",
        "        if word not in all_rus_words:\n",
        "            all_rus_words.add(word)\n",
        "\n",
        "# Vocabulary of English \n",
        "all_eng_words=set()\n",
        "for eng in lines.eng:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:10.826145Z",
          "start_time": "2020-06-15T12:19:10.781636Z"
        },
        "id": "yDir9XHsfpqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "e8781719-83a4-4e91-c480-0fe1ba073274"
      },
      "source": [
        "# Max Length of source sequence\n",
        "lenght_list=[]\n",
        "for l in lines.rus:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_src = np.max(lenght_list)\n",
        "max_length_src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:11.592387Z",
          "start_time": "2020-06-15T12:19:11.505642Z"
        },
        "id": "auV4wcK_fpq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "8c06e0e7-4433-4a90-c175-c3e977186b0a"
      },
      "source": [
        "# Max Length of target sequence\n",
        "lenght_list=[]\n",
        "for l in lines.eng:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_tar = np.max(lenght_list)\n",
        "max_length_tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:12.084084Z",
          "start_time": "2020-06-15T12:19:12.017022Z"
        },
        "id": "j6KshCf4fpq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "493f4835-547b-4425-bdf5-45c397569911"
      },
      "source": [
        "input_words = sorted(list(all_rus_words))\n",
        "target_words = sorted(list(all_eng_words))\n",
        "num_encoder_tokens = len(all_rus_words)\n",
        "num_decoder_tokens = len(all_eng_words)\n",
        "num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11582, 4207)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:12.599432Z",
          "start_time": "2020-06-15T12:19:12.592908Z"
        },
        "id": "0j11Tvwufpq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "3e5a6ed4-c43f-4228-cf4c-65c66870d7fa"
      },
      "source": [
        "num_decoder_tokens += 1 # For zero padding\n",
        "num_encoder_tokens +=1\n",
        "num_decoder_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:13.285461Z",
          "start_time": "2020-06-15T12:19:13.204485Z"
        },
        "id": "lDVGkrQEfpq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:13.816245Z",
          "start_time": "2020-06-15T12:19:13.788507Z"
        },
        "id": "Sw8OsDuzfpq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:14.721911Z",
          "start_time": "2020-06-15T12:19:14.697055Z"
        },
        "id": "oXXndRKQfpq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "8adb568f-507c-4950-b03c-2faf7f7793d7"
      },
      "source": [
        "lines.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>rus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37140</th>\n",
              "      <td>START_ he has curly hair _END</td>\n",
              "      <td>у него кудрявые волосы</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31613</th>\n",
              "      <td>START_ rabbits can swim _END</td>\n",
              "      <td>кролики могут плавать</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8596</th>\n",
              "      <td>START_ youre weird _END</td>\n",
              "      <td>вы странный</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23621</th>\n",
              "      <td>START_ theres a table _END</td>\n",
              "      <td>есть стол</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>START_ i beg you _END</td>\n",
              "      <td>умоляю вас</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33161</th>\n",
              "      <td>START_ tom is very open _END</td>\n",
              "      <td>том очень открыт</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19466</th>\n",
              "      <td>START_ do you have one _END</td>\n",
              "      <td>у тебя есть один</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9497</th>\n",
              "      <td>START_ i cried again _END</td>\n",
              "      <td>я опять заплакал</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33329</th>\n",
              "      <td>START_ tom may not come _END</td>\n",
              "      <td>том возможно не придёт</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4885</th>\n",
              "      <td>START_ tom is full _END</td>\n",
              "      <td>том наелся</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 eng                     rus\n",
              "37140  START_ he has curly hair _END  у него кудрявые волосы\n",
              "31613   START_ rabbits can swim _END   кролики могут плавать\n",
              "8596         START_ youre weird _END             вы странный\n",
              "23621     START_ theres a table _END               есть стол\n",
              "1045           START_ i beg you _END              умоляю вас\n",
              "33161   START_ tom is very open _END        том очень открыт\n",
              "19466    START_ do you have one _END        у тебя есть один\n",
              "9497       START_ i cried again _END        я опять заплакал\n",
              "33329   START_ tom may not come _END  том возможно не придёт\n",
              "4885         START_ tom is full _END              том наелся"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:20.217018Z",
          "start_time": "2020-06-15T12:19:20.186460Z"
        },
        "id": "szGJAm0zfpq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "b0d10e83-2082-45a5-e58d-f287c04c9ec2"
      },
      "source": [
        "#Split the data into train and test\n",
        "X, y = lines.rus, lines.eng\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34826,), (3870,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:26.732784Z",
          "start_time": "2020-06-15T12:19:26.722532Z"
        },
        "id": "AuHY-vShfprC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function which generates the data for train and test in batches\n",
        "\n",
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:28.312597Z",
          "start_time": "2020-06-15T12:19:28.309264Z"
        },
        "id": "gzmaoQq5fprE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the encoder-decoder architecture with GRU\n",
        "latent_dim = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:29.181525Z",
          "start_time": "2020-06-15T12:19:28.921869Z"
        },
        "id": "XjXjajVifprF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_gru = GRU(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h = encoder_gru(enc_emb)\n",
        "\n",
        "encoder_states = [state_h]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:30.144938Z",
          "start_time": "2020-06-15T12:19:29.870289Z"
        },
        "id": "ggQiV7bffprH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:31.164527Z",
          "start_time": "2020-06-15T12:19:31.095185Z"
        },
        "id": "ItExpxSufprI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:19:33.993626Z",
          "start_time": "2020-06-15T12:19:33.989212Z"
        },
        "id": "m6zVgrc1fprL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 32\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7qFs1vQcfm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/Colab Notebooks/weights_gru_ru-en.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-15T12:20:20.799119Z",
          "start_time": "2020-06-15T12:19:34.631953Z"
        },
        "id": "p6B83Y2yfprM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a020c37-59be-400e-81b2-7f1ea852b54a"
      },
      "source": [
        "print('Beginning Training!') \n",
        "current_time = datetime.datetime.now() \n",
        "\n",
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = 200,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)\n",
        "\n",
        "print(\"Training took time \", datetime.datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "200/200 [==============================] - 26s 132ms/step - loss: 1.8450 - acc: 0.4196 - val_loss: 1.7566 - val_acc: 0.4206\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 1.8038 - acc: 0.4272 - val_loss: 1.6996 - val_acc: 0.4316\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 1.7942 - acc: 0.4332 - val_loss: 1.7609 - val_acc: 0.4384\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 1.7749 - acc: 0.4378 - val_loss: 1.7585 - val_acc: 0.4443\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.7663 - acc: 0.4459 - val_loss: 1.7660 - val_acc: 0.4465\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.7229 - acc: 0.4500 - val_loss: 1.6547 - val_acc: 0.4539\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 29s 145ms/step - loss: 1.7030 - acc: 0.4536 - val_loss: 1.7195 - val_acc: 0.4591\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 1.6867 - acc: 0.4612 - val_loss: 1.5533 - val_acc: 0.4654\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 29s 144ms/step - loss: 1.6819 - acc: 0.4667 - val_loss: 1.7210 - val_acc: 0.4679\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 30s 148ms/step - loss: 1.6687 - acc: 0.4674 - val_loss: 1.6187 - val_acc: 0.4733\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 30s 150ms/step - loss: 1.6536 - acc: 0.4729 - val_loss: 1.7162 - val_acc: 0.4749\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 30s 150ms/step - loss: 1.6325 - acc: 0.4765 - val_loss: 1.7476 - val_acc: 0.4799\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 29s 146ms/step - loss: 1.6050 - acc: 0.4834 - val_loss: 1.5983 - val_acc: 0.4835\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 30s 152ms/step - loss: 1.6098 - acc: 0.4867 - val_loss: 1.6071 - val_acc: 0.4836\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 29s 146ms/step - loss: 1.6032 - acc: 0.4864 - val_loss: 1.6772 - val_acc: 0.4847\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 1.5991 - acc: 0.4938 - val_loss: 1.6218 - val_acc: 0.4901\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.5662 - acc: 0.4960 - val_loss: 1.5829 - val_acc: 0.4936\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.5519 - acc: 0.4989 - val_loss: 1.5241 - val_acc: 0.4948\n",
            "Epoch 19/100\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 1.5429 - acc: 0.5065 - val_loss: 1.4993 - val_acc: 0.4950\n",
            "Epoch 20/100\n",
            "200/200 [==============================] - 29s 143ms/step - loss: 1.5513 - acc: 0.5029 - val_loss: 1.6406 - val_acc: 0.4983\n",
            "Epoch 21/100\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 1.5287 - acc: 0.5103 - val_loss: 1.5507 - val_acc: 0.5007\n",
            "Epoch 22/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.5278 - acc: 0.5129 - val_loss: 1.5069 - val_acc: 0.5064\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 1.5152 - acc: 0.5147 - val_loss: 1.5248 - val_acc: 0.5073\n",
            "Epoch 24/100\n",
            "200/200 [==============================] - 30s 148ms/step - loss: 1.4888 - acc: 0.5238 - val_loss: 1.5053 - val_acc: 0.5096\n",
            "Epoch 25/100\n",
            "200/200 [==============================] - 30s 148ms/step - loss: 1.4965 - acc: 0.5219 - val_loss: 1.5922 - val_acc: 0.5100\n",
            "Epoch 26/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.4968 - acc: 0.5216 - val_loss: 1.4371 - val_acc: 0.5172\n",
            "Epoch 27/100\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 1.4947 - acc: 0.5311 - val_loss: 1.3529 - val_acc: 0.5170\n",
            "Epoch 28/100\n",
            "200/200 [==============================] - 29s 143ms/step - loss: 1.4703 - acc: 0.5298 - val_loss: 1.5362 - val_acc: 0.5186\n",
            "Epoch 29/100\n",
            "200/200 [==============================] - 29s 147ms/step - loss: 1.4647 - acc: 0.5374 - val_loss: 1.5286 - val_acc: 0.5228\n",
            "Epoch 30/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.4582 - acc: 0.5392 - val_loss: 1.5433 - val_acc: 0.5260\n",
            "Epoch 31/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4740 - acc: 0.5398 - val_loss: 1.4280 - val_acc: 0.5291\n",
            "Epoch 32/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4654 - acc: 0.5443 - val_loss: 1.7152 - val_acc: 0.5320\n",
            "Epoch 33/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.4664 - acc: 0.5456 - val_loss: 1.5851 - val_acc: 0.5318\n",
            "Epoch 34/100\n",
            "200/200 [==============================] - 30s 149ms/step - loss: 1.4553 - acc: 0.5496 - val_loss: 1.4553 - val_acc: 0.5374\n",
            "Epoch 35/100\n",
            "200/200 [==============================] - 30s 150ms/step - loss: 1.4476 - acc: 0.5557 - val_loss: 1.6047 - val_acc: 0.5386\n",
            "Epoch 36/100\n",
            "200/200 [==============================] - 30s 151ms/step - loss: 1.4578 - acc: 0.5532 - val_loss: 1.5594 - val_acc: 0.5420\n",
            "Epoch 37/100\n",
            "200/200 [==============================] - 26s 131ms/step - loss: 1.4617 - acc: 0.5546 - val_loss: 1.5882 - val_acc: 0.5434\n",
            "Epoch 38/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.4671 - acc: 0.5588 - val_loss: 1.3318 - val_acc: 0.5438\n",
            "Epoch 39/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4501 - acc: 0.5596 - val_loss: 1.5467 - val_acc: 0.5456\n",
            "Epoch 40/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.4370 - acc: 0.5673 - val_loss: 1.5386 - val_acc: 0.5474\n",
            "Epoch 41/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.4432 - acc: 0.5672 - val_loss: 1.3335 - val_acc: 0.5473\n",
            "Epoch 42/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.4546 - acc: 0.5652 - val_loss: 1.5293 - val_acc: 0.5509\n",
            "Epoch 43/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.4512 - acc: 0.5708 - val_loss: 1.5379 - val_acc: 0.5549\n",
            "Epoch 44/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4430 - acc: 0.5712 - val_loss: 1.5835 - val_acc: 0.5553\n",
            "Epoch 45/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4316 - acc: 0.5746 - val_loss: 1.3312 - val_acc: 0.5570\n",
            "Epoch 46/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4275 - acc: 0.5808 - val_loss: 1.6207 - val_acc: 0.5583\n",
            "Epoch 47/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4367 - acc: 0.5765 - val_loss: 1.6615 - val_acc: 0.5568\n",
            "Epoch 48/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4326 - acc: 0.5784 - val_loss: 1.6414 - val_acc: 0.5601\n",
            "Epoch 49/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4401 - acc: 0.5800 - val_loss: 1.6934 - val_acc: 0.5607\n",
            "Epoch 50/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.4174 - acc: 0.5829 - val_loss: 1.6857 - val_acc: 0.5622\n",
            "Epoch 51/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4035 - acc: 0.5887 - val_loss: 1.6708 - val_acc: 0.5635\n",
            "Epoch 52/100\n",
            "200/200 [==============================] - 30s 151ms/step - loss: 1.4115 - acc: 0.5894 - val_loss: 1.6429 - val_acc: 0.5644\n",
            "Epoch 53/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.4128 - acc: 0.5851 - val_loss: 1.6715 - val_acc: 0.5646\n",
            "Epoch 54/100\n",
            "200/200 [==============================] - 30s 149ms/step - loss: 1.4222 - acc: 0.5918 - val_loss: 1.5839 - val_acc: 0.5655\n",
            "Epoch 55/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.4003 - acc: 0.5918 - val_loss: 1.6967 - val_acc: 0.5695\n",
            "Epoch 56/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.3902 - acc: 0.5966 - val_loss: 1.3457 - val_acc: 0.5714\n",
            "Epoch 57/100\n",
            "200/200 [==============================] - 32s 160ms/step - loss: 1.3864 - acc: 0.5991 - val_loss: 1.4069 - val_acc: 0.5708\n",
            "Epoch 58/100\n",
            "200/200 [==============================] - 32s 161ms/step - loss: 1.3977 - acc: 0.5954 - val_loss: 1.3363 - val_acc: 0.5689\n",
            "Epoch 59/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.3915 - acc: 0.5990 - val_loss: 1.3554 - val_acc: 0.5727\n",
            "Epoch 60/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.3943 - acc: 0.6006 - val_loss: 1.4649 - val_acc: 0.5727\n",
            "Epoch 61/100\n",
            "200/200 [==============================] - 32s 162ms/step - loss: 1.3808 - acc: 0.6033 - val_loss: 1.6262 - val_acc: 0.5745\n",
            "Epoch 62/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.3619 - acc: 0.6085 - val_loss: 1.5285 - val_acc: 0.5754\n",
            "Epoch 63/100\n",
            "200/200 [==============================] - 32s 158ms/step - loss: 1.3752 - acc: 0.6049 - val_loss: 1.6193 - val_acc: 0.5733\n",
            "Epoch 64/100\n",
            "200/200 [==============================] - 32s 161ms/step - loss: 1.3730 - acc: 0.6033 - val_loss: 1.5989 - val_acc: 0.5763\n",
            "Epoch 65/100\n",
            "200/200 [==============================] - 32s 162ms/step - loss: 1.3899 - acc: 0.6097 - val_loss: 1.4909 - val_acc: 0.5786\n",
            "Epoch 66/100\n",
            "200/200 [==============================] - 32s 159ms/step - loss: 1.3563 - acc: 0.6109 - val_loss: 1.5005 - val_acc: 0.5801\n",
            "Epoch 67/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.3475 - acc: 0.6135 - val_loss: 1.6664 - val_acc: 0.5824\n",
            "Epoch 68/100\n",
            "200/200 [==============================] - 30s 151ms/step - loss: 1.3490 - acc: 0.6140 - val_loss: 1.1841 - val_acc: 0.5812\n",
            "Epoch 69/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.3638 - acc: 0.6107 - val_loss: 1.5543 - val_acc: 0.5804\n",
            "Epoch 70/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.3507 - acc: 0.6185 - val_loss: 1.4349 - val_acc: 0.5822\n",
            "Epoch 71/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.3557 - acc: 0.6176 - val_loss: 1.6566 - val_acc: 0.5836\n",
            "Epoch 72/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.3471 - acc: 0.6189 - val_loss: 1.5153 - val_acc: 0.5847\n",
            "Epoch 73/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.3262 - acc: 0.6242 - val_loss: 1.3891 - val_acc: 0.5847\n",
            "Epoch 74/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.3393 - acc: 0.6202 - val_loss: 1.6150 - val_acc: 0.5849\n",
            "Epoch 75/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.3365 - acc: 0.6205 - val_loss: 1.8177 - val_acc: 0.5884\n",
            "Epoch 76/100\n",
            "200/200 [==============================] - 29s 147ms/step - loss: 1.3538 - acc: 0.6247 - val_loss: 1.4704 - val_acc: 0.5873\n",
            "Epoch 77/100\n",
            "200/200 [==============================] - 29s 147ms/step - loss: 1.3228 - acc: 0.6265 - val_loss: 1.5327 - val_acc: 0.5872\n",
            "Epoch 78/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.3152 - acc: 0.6307 - val_loss: 1.6886 - val_acc: 0.5872\n",
            "Epoch 79/100\n",
            "200/200 [==============================] - 30s 150ms/step - loss: 1.3141 - acc: 0.6303 - val_loss: 1.4914 - val_acc: 0.5882\n",
            "Epoch 80/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.3279 - acc: 0.6273 - val_loss: 1.3592 - val_acc: 0.5894\n",
            "Epoch 81/100\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 1.3232 - acc: 0.6307 - val_loss: 1.4897 - val_acc: 0.5892\n",
            "Epoch 82/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.3248 - acc: 0.6316 - val_loss: 1.4266 - val_acc: 0.5904\n",
            "Epoch 83/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.3069 - acc: 0.6365 - val_loss: 1.3398 - val_acc: 0.5932\n",
            "Epoch 84/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.2981 - acc: 0.6391 - val_loss: 1.6520 - val_acc: 0.5909\n",
            "Epoch 85/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.3071 - acc: 0.6353 - val_loss: 1.4773 - val_acc: 0.5919\n",
            "Epoch 86/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.3089 - acc: 0.6344 - val_loss: 1.6723 - val_acc: 0.5943\n",
            "Epoch 87/100\n",
            "200/200 [==============================] - 31s 156ms/step - loss: 1.3203 - acc: 0.6378 - val_loss: 1.3076 - val_acc: 0.5945\n",
            "Epoch 88/100\n",
            "200/200 [==============================] - 31s 153ms/step - loss: 1.2960 - acc: 0.6403 - val_loss: 1.4932 - val_acc: 0.5920\n",
            "Epoch 89/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.2819 - acc: 0.6439 - val_loss: 1.6247 - val_acc: 0.5951\n",
            "Epoch 90/100\n",
            "200/200 [==============================] - 30s 149ms/step - loss: 1.2887 - acc: 0.6435 - val_loss: 1.4260 - val_acc: 0.5953\n",
            "Epoch 91/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.2997 - acc: 0.6386 - val_loss: 1.3699 - val_acc: 0.5966\n",
            "Epoch 92/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.2970 - acc: 0.6452 - val_loss: 1.3544 - val_acc: 0.5974\n",
            "Epoch 93/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.2902 - acc: 0.6448 - val_loss: 1.1726 - val_acc: 0.5984\n",
            "Epoch 94/100\n",
            "200/200 [==============================] - 30s 149ms/step - loss: 1.2756 - acc: 0.6485 - val_loss: 1.3653 - val_acc: 0.5996\n",
            "Epoch 95/100\n",
            "200/200 [==============================] - 30s 150ms/step - loss: 1.2730 - acc: 0.6512 - val_loss: 1.6227 - val_acc: 0.5972\n",
            "Epoch 96/100\n",
            "200/200 [==============================] - 30s 152ms/step - loss: 1.2855 - acc: 0.6452 - val_loss: 1.6328 - val_acc: 0.5975\n",
            "Epoch 97/100\n",
            "200/200 [==============================] - 31s 155ms/step - loss: 1.2809 - acc: 0.6472 - val_loss: 1.4543 - val_acc: 0.6015\n",
            "Epoch 98/100\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.2942 - acc: 0.6494 - val_loss: 1.7590 - val_acc: 0.6010\n",
            "Epoch 99/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.2653 - acc: 0.6538 - val_loss: 1.3966 - val_acc: 0.6027\n",
            "Epoch 100/100\n",
            "200/200 [==============================] - 31s 157ms/step - loss: 1.2563 - acc: 0.6546 - val_loss: 1.3816 - val_acc: 0.6004\n",
            "Training took time  0:50:17.431173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-3D6DmL5Vdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('/content/drive/My Drive/Colab Notebooks/weights_gru_ru-en.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWUZPq-UfprQ",
        "colab_type": "text"
      },
      "source": [
        "### Inference Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAKdZjgNfprR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2 = decoder_gru(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "074ATvPsfprV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finally, generate the output by defining the following function and later calling it.\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak0yGzP0fprX",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation on Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvYXFQpYfprX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R1GwN-u6ZH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "8637a153-4216-40b3-90bc-4efb5be5bcfe"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('\\033[1m'+ 'Input Russian sentence:' +'\\033[0m', X_train[k:k+1].values[0])\n",
        "print('\\033[1m'+ 'Actual English translation:' +'\\033[0m', y_train[k:k+1].values[0][6:-4])\n",
        "print('\\033[1m'+ 'Predicted English translation:' +'\\033[0m', decoded_sentence[:-4])\n",
        "nubia.score( y_train[k:k+1].values[0][6:-4], decoded_sentence[:-4], verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mInput Russian sentence:\u001b[0m пожалуйста прекрати это\n",
            "\u001b[1mActual English translation:\u001b[0m  please stop it \n",
            "\u001b[1mPredicted English translation:\u001b[0m  its a lot \n",
            "Semantic relation: 1.555252194404602/5.0\n",
            "Percent chance of contradiction: 1.7395731061697006%\n",
            "Percent chance of irrelevancy or new information: 66.66330695152283%\n",
            "Percent chance of logical agreement: 31.59712255001068%\n",
            "\n",
            "NUBIA score: 0.29142747307207184/1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29142747307207184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRlbQTh-60IM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "02a6a370-9797-46c4-b4bc-caad62eecd87"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('\\033[1m'+ 'Input Russian sentence:' +'\\033[0m', X_train[k:k+1].values[0])\n",
        "print('\\033[1m'+ 'Actual English translation:' +'\\033[0m', y_train[k:k+1].values[0][6:-4])\n",
        "print('\\033[1m'+ 'Predicted English translation:' +'\\033[0m', decoded_sentence[:-4])\n",
        "nubia.score( y_train[k:k+1].values[0][6:-4], decoded_sentence[:-4], verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mInput Russian sentence:\u001b[0m стопудово\n",
            "\u001b[1mActual English translation:\u001b[0m  yes of course \n",
            "\u001b[1mPredicted English translation:\u001b[0m  he was up \n",
            "Semantic relation: 1.980818510055542/5.0\n",
            "Percent chance of contradiction: 2.031507156789303%\n",
            "Percent chance of irrelevancy or new information: 91.29781126976013%\n",
            "Percent chance of logical agreement: 6.670675426721573%\n",
            "\n",
            "NUBIA score: 0.2985636073411566/1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2985636073411566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx1mM4W8fpsv",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation on Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0I34CZxfpsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
        "k=-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3GB2z0d6bi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "f3fd5099-f49f-4446-ce5e-74e3ea73625e"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('\\033[1m'+ 'Input Russian sentence:' +'\\033[0m', X_test[k:k+1].values[0])\n",
        "print('\\033[1m'+ 'Actual English translation:' +'\\033[0m', y_test[k:k+1].values[0][6:-4])\n",
        "print('\\033[1m'+ 'Predicted English translation:' +'\\033[0m', decoded_sentence[:-4])\n",
        "nubia.score( y_test[k:k+1].values[0][6:-4], decoded_sentence[:-4], verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mInput Russian sentence:\u001b[0m мы заблудились\n",
            "\u001b[1mActual English translation:\u001b[0m  we got lost \n",
            "\u001b[1mPredicted English translation:\u001b[0m  were lost \n",
            "Semantic relation: 3.7291259765625/5.0\n",
            "Percent chance of contradiction: 0.26536115910857916%\n",
            "Percent chance of irrelevancy or new information: 1.0849418118596077%\n",
            "Percent chance of logical agreement: 98.64969849586487%\n",
            "\n",
            "NUBIA score: 0.5906952187102021/1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5906952187102021"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4WofOIq6c23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "0aa38df8-24fd-4170-8f68-5324ea246810"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('\\033[1m'+ 'Input Russian sentence:' +'\\033[0m', X_test[k:k+1].values[0])\n",
        "print('\\033[1m'+ 'Actual English translation:' +'\\033[0m', y_test[k:k+1].values[0][6:-4])\n",
        "print('\\033[1m'+ 'Predicted English translation:' +'\\033[0m', decoded_sentence[:-4])\n",
        "nubia.score( y_test[k:k+1].values[0][6:-4], decoded_sentence[:-4], verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mInput Russian sentence:\u001b[0m они доктора\n",
            "\u001b[1mActual English translation:\u001b[0m  theyre doctors \n",
            "\u001b[1mPredicted English translation:\u001b[0m  they are a lot \n",
            "Semantic relation: 1.435977816581726/5.0\n",
            "Percent chance of contradiction: 4.46191169321537%\n",
            "Percent chance of irrelevancy or new information: 89.06938433647156%\n",
            "Percent chance of logical agreement: 6.468704342842102%\n",
            "\n",
            "NUBIA score: 0.32035303384572267/1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32035303384572267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeBaBYr-gwjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "bf71c432-0d1e-41b3-a6b9-a914864c9fc9"
      },
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(val_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('\\033[1m'+ 'Input Russian sentence:' +'\\033[0m', X_test[k:k+1].values[0])\n",
        "print('\\033[1m'+ 'Actual English translation:' +'\\033[0m', y_test[k:k+1].values[0][6:-4])\n",
        "print('\\033[1m'+ 'Predicted English translation:' +'\\033[0m', decoded_sentence[:-4])\n",
        "nubia.score( y_test[k:k+1].values[0][6:-4], decoded_sentence[:-4], verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mInput Russian sentence:\u001b[0m как том мог выиграть\n",
            "\u001b[1mActual English translation:\u001b[0m  how could tom win \n",
            "\u001b[1mPredicted English translation:\u001b[0m  how did tom win \n",
            "Semantic relation: 4.144294738769531/5.0\n",
            "Percent chance of contradiction: 1.8374454230070114%\n",
            "Percent chance of irrelevancy or new information: 2.0112859085202217%\n",
            "Percent chance of logical agreement: 96.15126848220825%\n",
            "\n",
            "NUBIA score: 0.8534810744294751/1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8534810744294751"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    }
  ]
}